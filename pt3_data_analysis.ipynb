{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dqtma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dqtma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 312 ms (started: 2023-07-03 11:31:12 -04:00)\n"
     ]
    }
   ],
   "source": [
    "# standard modules\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import time\n",
    "import wget\n",
    "from tqdm import tqdm, trange\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy.engine import URL\n",
    "from IPython.display import display\n",
    "import itertools\n",
    "\n",
    "# data science modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression #import classifiers from sklearn\n",
    "from sklearn import metrics #import different metrics to evaluate the classifiers\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.manifold import TSNE #scikit learn's TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# nlp modules\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# imports deep learning modules\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from pytorch_transformers import BertModel\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import torch as torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# custom\n",
    "from sql_alchemy_utility.sql_alchemy_utility import SqlOperations as Sql\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autotime\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "nltk.download('punkt') # download Punkt sentence tokenization models\n",
    "nltk.download('stopwords') # download stopwords\n",
    "# pd.set_option('display.max_colwidth', None) # set max width of displayer columns to none to see entire transciptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "query = \"SELECT * FROM noteevents_05012023\"\n",
    "\n",
    "df_dd = Sql.load_data_from_db(query)\n",
    "\n",
    "df_dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## keep track of viable records\n",
    "\n",
    "# viable records that show the patient receiving proper VTE interventions\n",
    "gold_inter = [192, 47760, 44381, 5410]\n",
    "\n",
    "# viable records that shows a contraindication for VTE\n",
    "gold_cont = [1846, 2741, 3675, 4558, 7224, 5909, 10840, 9438, 10111, 10715, 10048\n",
    "    , 616712, 534, 1945, 7201, 10849, 8785, 16097, 25055, 40481, 45948\n",
    "    , 46269, 46599, 46599, 33873, 52421, 52477, 43083, 54603, 49972, 327873\n",
    "    , 2542, 4252, 5410, 5409, 7892, 7892, 6291, 8877, 8878, 8879]\n",
    "\n",
    "# viable records that show a contraindication for non-VTE\n",
    "silver_cont = [1258221]\n",
    "\n",
    "# non-viable records\n",
    "not_rel = [56, 979]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active learning dataframe\n",
    "df_bs = df_dd\n",
    "\n",
    "df_bs[\"label\"] =  \"\" #not labeled yet\n",
    "df_bs[\"label\"] =  np.where(df_bs[\"row_id\"].isin(not_rel), \"nonrelevant\", df_bs[\"label\"]) \n",
    "df_bs[\"label\"] =  np.where(df_bs[\"row_id\"].isin(gold_inter), \"intervention\", df_bs[\"label\"])\n",
    "df_bs[\"label\"] =  np.where(df_bs[\"row_id\"].isin(gold_cont), \"contraindication\", df_bs[\"label\"])\n",
    "\n",
    "# drop nan transcription to not break model\n",
    "df_bs = df_bs.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate gold from the rest.\n",
    "shortlist = ['contraindication', 'intervention', 'nonrelevant']\n",
    "\n",
    "gold = df_bs[df_bs['label'].isin(shortlist)]\n",
    "rest = df_bs[~df_bs['label'].isin(shortlist)]\n",
    "\n",
    "print('gold:', gold.shape)\n",
    "print('rest:', rest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set stopwords such as \"a\", \"the\", etc.\n",
    "mystopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Function to tokenize text\n",
    "def preprocess_corpus(texts):\n",
    "    #initalize English stopwords\n",
    "    mystopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def remove_stops_digits(tokens):\n",
    "        # Nested function that removes stopwords and digits from a list of tokens\n",
    "        # return [token.lower() for token in tokens if token.lower() not in mystopwords and not token.isdigit()]\n",
    "        return [token.lower() for token in tokens if token.lower() and token not in string.punctuation not in mystopwords and not token.isdigit()]\n",
    "    \n",
    "    # This return statement below uses the above function to process  tokenizer output further. \n",
    "    return [remove_stops_digits(word_tokenize(content)) for content in texts]\n",
    "\n",
    "# Use gold data for train and rest for prediction (test)\n",
    "train_content = preprocess_corpus(gold['text'])\n",
    "train_cats = gold['label']\n",
    "\n",
    "test_content = preprocess_corpus(rest['text'])\n",
    "test_cats = rest['label']\n",
    "\n",
    "print(\"length of train data:\", len(train_content), len(train_cats))\n",
    "print(\"length of test data:\", len(test_content), len(test_cats))\n",
    "\n",
    "# print example tokenized sentence\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (train_content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# prepare training data in doc2vec format:\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m train_doc2vec \u001B[38;5;241m=\u001B[39m [TaggedDocument((d), tags\u001B[38;5;241m=\u001B[39m[\u001B[38;5;28mstr\u001B[39m(i)]) \u001B[38;5;28;01mfor\u001B[39;00m i, d \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[43mtrain_content\u001B[49m)]\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Train a doc2vec model to learn representations\u001B[39;00m\n\u001B[0;32m      5\u001B[0m model \u001B[38;5;241m=\u001B[39m Doc2Vec(vector_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m, alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.025\u001B[39m, min_count\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, dm \u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'train_content' is not defined"
     ]
    }
   ],
   "source": [
    "# prepare training data in doc2vec format:\n",
    "train_doc2vec = [TaggedDocument((d), tags=[str(i)]) for i, d in enumerate(train_content)]\n",
    "\n",
    "# Train a doc2vec model to learn representations\n",
    "model = Doc2Vec(vector_size=50, alpha=0.025, min_count=5, dm =1, epochs=100)\n",
    "model.build_vocab(train_doc2vec)\n",
    "model.train(train_doc2vec, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer the feature representation for training and test data using the trained model\n",
    "model= Doc2Vec.load(\"d2v.model\")\n",
    "\n",
    "# infer in multiple epochs to get a stable representation. \n",
    "train_vectors =  [model.infer_vector(list_of_tokens) for list_of_tokens in train_content]\n",
    "test_vectors = [model.infer_vector(list_of_tokens) for list_of_tokens in test_content]\n",
    "\n",
    "# Use a logistic regression regular classifier\n",
    "myclass = LogisticRegression(class_weight=\"balanced\", max_iter=1000) #because classes are not balanced. high iterations to converge successfully\n",
    "myclass.fit(train_vectors, train_cats)\n",
    "\n",
    "# Make predictions off the rest of the dataset(non-gold) to do bootstrapping\n",
    "preds = myclass.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the selected contraindications\n",
    "value = 'contraindication'\n",
    "print(f\"number of predicted {value}:{len(preds[preds == value])}\")\n",
    "for i in range(len(preds)):\n",
    "    if preds[i] == value:\n",
    "        print(f\"id:{rest['id'].iloc[i]}, pred:{preds[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the selected interventions\n",
    "value = 'interventions'\n",
    "print(f\"number of predicted {value}:{len(preds[preds == value])}\")\n",
    "for i in range(len(preds)):\n",
    "    if preds[i] == value:\n",
    "        print(f\"id:{rest['id'].iloc[i]}, pred:{preds[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the selected nonrelevant\n",
    "limit = 10\n",
    "value = 'nonrelevant'\n",
    "print(f\"number of predicted {value}:{len(preds[preds == value])}\")\n",
    "for i in range(limit):\n",
    "    if preds[i] == value:\n",
    "        print(f\"id:{rest['id'].iloc[i]}, pred:{preds[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examined record with print out transcription function\n",
    "record_id = 2467\n",
    "printOutTran(record_id, df_dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new labedled data \n",
    "gold_interv_b = gold_inter + [9,71,749]\n",
    "gold_contrain_b = gold_cont + []\n",
    "not_rel_b = not_rel + [913,970,1104,1216,1471,1575,1737,1756,2328,2465,2466,2467,2468,2481,2622,2821,2891,2993,3240,3261,4027,4129,4223,4232,4269,4665]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
