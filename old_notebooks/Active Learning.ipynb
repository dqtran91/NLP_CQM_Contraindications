{"cells":[{"cell_type":"code","source":["'''set the environment'''\n#import needed packages\nfrom pyspark.sql.functions import *\nfrom pyspark.sql import *\nfrom pyspark.sql.types import *\nimport fnmatch\n\n#data path\npath = \"\"\n\n\n#create and set database\n# spark.sql(f\"DROP DATABASE IF EXISTS {database} CASCADE\")\n# spark.sql(f\"CREATE DATABASE {database}\")\nspark.sql(f\"USE {database}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4e07be0-bad6-4b13-b776-d19158c87f44"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["!pip install gensim"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25213685-cdd9-4f09-b168-43e7a3f78f7f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\nimport nltk\nnltk.download('stopwords')\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f73fba0-b404-4899-9a98-7113db784f82"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["df = spark.table('textTable').toPandas()\nprint('textTable shape:', df.shape)\ndf.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1004605d-f2dd-4a8d-8627-5d5bcaab77e1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#separte gold from the rest.\nshortlist = ['contraindication', 'no contraindication']\n\ngold = df[df['label'].isin(shortlist)]\nrest = df[~df['label'].isin(shortlist)]\n\nprint('gold:', gold.shape)\nprint('rest:', rest.shape)\nprint('the type:', type(gold))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4036158d-4e15-45b9-8503-4b574ae4fbb4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#strip_handles removes personal information such as twitter handles, which don't\n#contribute to emotion in the tweet. preserve_case=False converts everything to lowercase.\ntweeter = TweetTokenizer(strip_handles=True,preserve_case=False)\nmystopwords = set(stopwords.words(\"english\"))\n\n#Function to tokenize tweets, remove stopwords and numbers. \n#Keeping punctuations and emoticon symbols could be relevant for this task!\ndef preprocess_corpus(texts):\n    def remove_stops_digits(tokens):\n        #Nested function that removes stopwords and digits from a list of tokens\n        return [token for token in tokens if token not in mystopwords and not token.isdigit()]\n    #This return statement below uses the above function to process twitter tokenizer output further. \n    return [remove_stops_digits(tweeter.tokenize(content)) for content in texts]\n\n#Split data into train and test\ntrain_content = preprocess_corpus(gold['text'])\ntrain_cats = gold['label']\n\ntest_content = preprocess_corpus(rest['text'])\ntest_cats = rest['label']\n\nprint(\"length of train data:\", len(train_content), len(train_cats))\nprint(\"length of test data:\", len(test_content), len(test_cats))\nprint('the type of the content:', type(test_content))\nprint('the type of the cats:', type(test_cats))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19b3c843-74b5-4cfc-831b-e204e2e8a0e7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#read data\ni = 10\nprint(rest.iloc[i])\nprint(test_cats.iloc[i])\nprint(test_content[i])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"baca871f-5b46-4c4e-b8d1-83f4a028f5ad"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#prepare training data in doc2vec format:\ntrain_doc2vec = [TaggedDocument((d), tags=[str(i)]) for i, d in enumerate(train_content)]\n#Train a doc2vec model to learn tweet representations. Use only training data!!\nmodel = Doc2Vec(vector_size=50, alpha=0.025, min_count=5, dm =1, epochs=100)\nmodel.build_vocab(train_doc2vec)\nmodel.train(train_doc2vec, total_examples=model.corpus_count, epochs=model.epochs)\nmodel.save(\"d2v.model\")\nprint(\"Model Saved\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d0da4e46-698e-4254-bbfa-7e996721ce22"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#Infer the feature representation for training and test data using the trained model\nmodel= Doc2Vec.load(\"d2v.model\")\n#infer in multiple steps to get a stable representation. \ntrain_vectors =  [model.infer_vector(list_of_tokens, steps=50) for list_of_tokens in train_content]\ntest_vectors = [model.infer_vector(list_of_tokens, steps=50) for list_of_tokens in test_content]\n\n#Use any regular classifier like logistic regression\nfrom sklearn.linear_model import LogisticRegression\n\nmyclass = LogisticRegression(class_weight=\"balanced\") #because classes are not balanced. \nmyclass.fit(train_vectors, train_cats)\n\npreds = myclass.predict(test_vectors)\nfrom sklearn.metrics import classification_report, confusion_matrix\n# print(classification_report(test_cats, preds))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e39f31e6-744b-4457-ad55-754a35ff976c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["print('the type of preds:', type(preds))\n\nvalue = 'contraindication'\nprint(f\"number of predicted {value}:{len(preds[preds == value])}\")\nfor i in range(len(preds)):\n  if preds[i] == value:\n    print(f\"index:{i}, filename:{rest['fileName'].iloc[i]}, pred:{preds[i]}\")\n    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7acd59f-d1ae-4d91-86b5-207972ad350e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Active Learning","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2128806262638397}},"nbformat":4,"nbformat_minor":0}
